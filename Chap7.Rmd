## 1. Introduction

__The performance of a learning method relates to its prediction capability on independent test data__

## 2. Bias, Variance, and MOdel Complexity


* Training Error $\rightarrow$ * Test Error $\rightarrow$ * Expected test error

### 2.1 Regression

* __Test error__: aka _generalization error_, is the prediction error over an independent test sample $\text{Err}_\tau = E{L(Y, \hat{f}(x))|\tau}$. $\tau$ represents a fixed training set. (Error in population based on a fixed training set)

* __Expected prediction error__: or _expected test error_ $\text{Err} = E[L(Y,\hat{f}(X))] = E[\text{Err}_{\tau}]$. Expectation averages over everything that is random, including the randomness in the training set that produced $\hat{f}$. (Error of the model itself on average) 

* __Training error__: is the average loss over the training sample $\bar{\text{err}}= \frac{1}{N}\sum_{i=1}^N L(y_i, \hat{f}(x_i))$ (Error of the training set)

Training error, decreasing over model complexity, is not a good estimate of test error.  


### 2.2 Classification

Loss function can be 0-1 loss or $-2\times$log likelihood (aka deviance.)

For response $G$ with $G = 1, 2, 3, ...K$ as the categories and predictor $X$, we model the probability $p_k(X) =  \text{Pr}(G = k|X)$, using $\hat{G}(X) = \text{argmax}_k\hat{p}_k(X)$. 

$L(G, \hat{p}(X)) = -2\sum_kI(G=k)\log \hat{p}_k (X) = -1\log\hat{p}_G(X)$, i.e. $-2 \times$ log likelihood.

* __Test error__: $\text{Err}_\tau = E{L(G, \hat{G}(x))|\tau}$

* __Training error__: $\bar{\text{err}} = -\frac{2}{N}\sum_i \log\hat{p}_{g_i}(x_i)$, the sampling log-likelihood (to be minimized as there is the -2)


__In general__, $L(Y,\theta(X)) = -2\cdot \log\text{Pr}_{\theta(X)}(Y)$, the -2 times of the log likeliihood can be used a s loss function for general response densities, such as Poisoon, gamma, exponential, log-normal, and others. 


__Goal:__ For some tuning parameter $\alpha$ that determines the prediction $\hat{f}_\alpha(x)$, that minimizes the error, that is produces the minimum of the __average test error__. 

### 2.3 Split of data

Training (fit the model), validation (model selection), and test (model assessment, 25% typically). 


Validation can be done anlytically (AIC, BIC, MDL, SRM), or by efficient re-use (cross validation and bootstrap). 

## 3. Bia-Variance Decomposition

The expected prediction error of a regression fit $\hat{f}(x)$ at input $X = x_0$
$$\begin{aligned}
\text{Err}(x_0) &= E[(Y - \hat{f}(x_0))^2|X=x_0]\\
&= \sigma^2_\epsilon + [E\hat{f}(x_0) - f(x_0)]^2 + E[\hat{f}(x_0) - E\hat{f}(x_0)]^2\\
&= \sigma^2_\epsilon + \text{Bias}^2(\hat{f}(x_0)) + \text{Var}(\hat{f}(x_0))\\
&= \text{Irreducible error} + \text{Bias}^2 + \text{Variance}
\end{aligned}$$


Bias-variance tradeoff behaves differently for 0-1 loss that it does for squared error loss. If our estimate is $p = 0.6$, as compared to true $p = 0.9$, even if the squared bias is significant, but we still have made correct decision.

## 4. The Optimism of the Training Error Rate

Define the training set $\tau = \{(x_1,y_1),(x_2,y_2),\dots, (x_N,y_N)\}$ (fixed), and new test data from the joint distribution $(X^0, Y^0)$. 

The test error/ generalization error is $\text{Err}_\tau = E_{X^0,Y^0}[{L(Y^0, \hat{f}(X^0))|\tau}]$

The expected test/prediction error is $\text{Err} = E_{\tau}E_{X^0,Y^0}[{L(Y^0, \hat{f}(X^0))|\tau}]$, that is averging over training sets

The training error is $\bar{\text{err}}= \frac{1}{N}\sum_{i=1}^N L(y_i, \hat{f}(x_i))$

The training error is always going to be smaller than the test error, i.e. training error is overly optimistic. 

The expected optimism can be represented as $\omega = \frac{2}{N}\sum_{i = 1}^N \text{Cov}(\hat{y}_i, y_i)$, i.e. how strongly $y_i$ affects its own prediction. If we fit the data too hard, then the optimism is large. 


To estimate expected prediction error from training error:

* Estimate the optimism (can be easily done for linear model, $\omega = \frac{2d}{N}\sigma^2_{\epsilon}$),  and add it to the training error

* Do cross-validation or bootstrap to get direct estimate of extra sample error $\text{Err}$. (applicable to any loss function without linear assumption)

* In sample error rate is used for model selection. 

https://www.stat.cmu.edu/~ryantibs/advmethods/notes/errval.pdf


Sections 5 - 7 are about in sample error estimation for model selection

## 5. Estimates of In-Sample Prediction Error

$C_p = \bar{\text{err}} + 2 \frac{d}{N}\hat{\sigma}^2_\epsilon$, $d$ is the number of inputs or basis functions

$AIC = -\frac{2}{N}\text{loglik} + 2\frac{d}{N}$, where $\text{loglik} = \sum_{i=1}^N\log\text{Pr}_{\hat{\theta}}(y_i)$, i.e. the maximized log-likelihood, and $d$ is model complexity. (Used when the fitting is carried out by maximization of a likelihood)

$AIC(\alpha) = \bar{\text{err}}(\alpha) + 2 \frac{d(\alpha)}{N}\hat{\sigma}^2_\epsilon$ for models with tuning parameter $\alpha$

## 6. Effective Number of Parameters
 
For a linear fitting model $\hat{y} = Sy$ including linear regression on the original features or a derived basis set, and smoothing methods that use quadratic shrinkage, such as ridge regression and cubic smoothing splines, the effective number of parameters is defined as $df(S) = \text{trace}(S)$. 

## 7. The Bayesian Approach and BIC

General form $\text{BIC} = -2\text{loglik} + (\log N)d$

AIC tends to choose models with are too complex when $N \rightarrow \infty$, and for finite samples BIC often chooses models that are too simply due to its heavy penalty on complexity

## 8. Minimum Description Length

## 10. Cross Validation

Goal: To estimate prediction error. It estimates the expected extra-sample error $\text{Err} = E[L(Y,\hat{f}(X))]$

### 10.1 K Fold CV

Split the data into $K$ parts with equal size. For the $k$th part, we fit the model to the other $K-1$ parts of the data and calculate the prediction error of the fitted model when predicting the $k$th part of the data. We do this for $k = 1,2,...K$ and combine the $K$ estimates of the prediction errors. 

$\mathcal{k}:\{1...N\} \rightarrow \{1...K\}$ represents the partition. $\hat{f}^{-k}(x)$ denoted the fitted value while excluding the set $k$. The cross validation estimate of the prediction error is 

$$\text{CV}(\hat{f}) = \frac{1}{N}\sum_{i=1}^NL(y_i - \hat{f}^{-k}(x))$$

or 
$$\text{CV}(\hat{f}) = \frac{1}{K}\sum_{k=1}^K [\frac{1}{n_k}\sum_{i \in \text{kth val set}}L(y_i - \hat{f}^{-k}(x))]$$

LOCV: Low bias but high variance, also high computational burden.  

__One Standard Error Rule__ Choose the most parsimonious model whose error is no more than one standard error above the error of the best model. 

__Generalized Cross Validation__ A convenient approximation to leave one out CV, for linear fitting under squared error loss. 

$$GCV(\hat{f}) = \frac{1}{N}\sum_i [\frac{y_i - \hat{f}(x_i)}{1-\text{trace}(\mathbf{S})/N}]^2$$
### 10.2 Tips for CV

DON'T subset the data before CV, because the subset was chosen on the basis of all of the samples. Leaving samples out after the subset does not correctly mimic the application of the classifier. 

In general, with a multistep modeling procedure, cross validation must be applied to the entire sequence of modeling steps. 


## 11. Bootstrap





