## 1. Introduction

__The performance of a learning method relates to its prediction capability on independent test data__

## 2. Bias, Variance, and MOdel Complexity

### 2.1 Regression

* __Test error__: aka _generalization error_, is the prediction error over an independent test sample $\text{Err}_\tau = E{L(Y, \hat{f}(x))|\tau}$. $\tau$ represents a fixed training set. (Error in population based on a fixed training set)

* __Expected prediction error__: or _expected test error_ $\text{Err} = E[L(Y,\hat{f}(X))] = E[\text{Err}_{\tau}]$. Expectation averages over everything that is random, including the randomness in the training set that produced $\hat{f}$. (Error of the model itself on average) 

* __Training error__: is the average loss over the training sample $\bar{\text{err}}= \frac{1}{N}\sum_{i=1}^N L(y_i, \hat{f}(x_i))$ (Error of the training set)

Training error, decreasing over model complexity, is not a good estimate of test error.  


### 2.2 Classification

Loss function can be 0-1 loss or $-2\times$log likelihood (aka deviance.)

For response $G$ with $G = 1, 2, 3, ...K$ as the categories and predictor $X$, we model the probability $p_k(X) =  \text{Pr}(G = k|X)$, using $\hat{G}(X) = \text{argmax}_k\hat{p}_k(X)$. 

$L(G, \hat{p}(X)) = -2\sum_kI(G=k)\log \hat{p}_k (X) = -1\log\hat{p}_G(X)$, i.e. $-2 \times$ log likelihood.

* __Test error__: $\text{Err}_\tau = E{L(G, \hat{G}(x))|\tau}$

* __Training error__: $\bar{\text{err}} = -\frac{2}{N}\sum_i \log\hat{p}_{g_i}(x_i)$, the sampling log-likelihood (to be minimized as there is the -2)


__In general__, $L(Y,\theta(X)) = -2\cdot \log\text{Pr}_{\theta(X)}(Y)$, the -2 times of the log likeliihood can be used a s loss function for general response densities, such as Poisoon, gamma, exponential, log-normal, and others. 


__Goal:__ For some tuning parameter $\alpha$ that determines the prediction $\hat{f}_\alpha(x)$, that minimizes the error, that is produces the minimum of the __average test error__. 

### 2.3 Split of data

Training (fit the model), validation (model selection), and test (model assessment, 25% typically). 


Validation can be done anlytically (AIC, BIC, MDL, SRM), or by efficient re-use (cross validation and bootstrap). 

## 3. Bia-Variance Decomposition

The expected prediction error of a regression fit $\hat{f}(x)$ at input $X = x_0$
$$\begin{aligned}
\text{Err}(x_0) &= E[(Y - \hat{f}(x_0))^2|X=x_0]\\
&= \sigma^2_\epsilon + [E\hat{f}(x_0) - f(x_0)]^2 + E[\hat{f}(x_0) - E\hat{f}(x_0)]^2\\
&= \sigma^2_\epsilon + \text{Bias}^2(\hat{f}(x_0)) + \text{Var}(\hat{f}(x_0))\\
&= \text{Irreducible error} + \text{Bias}^2 + \text{Variance}
\end{aligned}$$


Bias-variance tradeoff behaves differently for 0-1 loss that it does for squared error loss. If our estimate is $p = 0.6$, as compared to true $p = 0.9$, even if the squared bias is significant, but we still have made correct decision.

## 4. The Optimism of the Training Error Rate

## 5. Estimates of In-Sample Prediction Error

$C_p = \bar{\text{err}} + 2 \frac{d}{N}\hat{\sigma}^2_\epsilon$, $d$ is the number of inputs or basis functions

$AIC = -\frac{2}{N}\text{loglik} + 2\frac{d}{N}$

$AIC(\alpha) = \bar{\text{err}}(\alpha) + 2 \frac{d(\alpha)}{N}\hat{\sigma}^2_\epsilon$ for models with tuning parameter $\alpha$

## 6. Effective Number of Parameters

## 7. The Bayesian Approach and BIC

General form $\text{BIC} = -2\text{loglik} + (\log N)d$

## 10. Cross Validation

Goal: To estimate prediction error. It estimates the expected extra-sample error $\text{Err} = E[L(Y,\hat{f}(X))]$

### 10.1 K Fold CV

Split the data into $K$ parts with equal size. For the $k$th part, we fit the model to the other $K-1$ parts of the data and calculate the prediction error of the fitted model when predicting the $k$th part of the data. We do this for $k = 1,2,...K$ and combine the $K$ estimates of the prediction errors. 

$\mathcal{k}:\{1...N\} \rightarrow \{1...K\}$ represents the partition. $\hat{f}^{-k}(x)$ denoted the fitted value while excluding the set $k$. The cross validation estimate of the prediction error is 

$$\text{CV}(\hat{f}) = \frac{1}{N}\sum_{i=1}^NL(y_i - \hat{f}^{-k}(x))$$

or 
$$\text{CV}(\hat{f}) = \frac{1}{K}\sum_{k=1}^K [\frac{1}{n_k}\sum_{i \in \text{kth val set}}L(y_i - \hat{f}^{-k}(x))]$$

LOCV: Low bias but high variance, also high computational burden.  

__One Standard Error Rule__ Choose the most parsimonious model whose error is no more than one standard error above the error of the best model. 

__Generalized Cross Validation__ A convenient approximation to leave one out CV, for linear fitting under squared error loss. 

$$GCV(\hat{f}) = \frac{1}{N}\sum_i [\frac{y_i - \hat{f}(x_i)}{1-\text{trace}(\mathbf{S})/N}]^2$$
### 10.2 Tips for CV

DON'T subset the data before CV, because the subset was chosen on the basis of all of the samples. Leaving samples out after the subset does not correctly mimic the application of the classifier. 

In general, with a multistep modeling procedure, cross validation must be applied to the entire sequence of modeling steps. 


## 11. Bootstrap





